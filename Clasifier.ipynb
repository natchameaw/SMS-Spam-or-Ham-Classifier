{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv \n",
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "from nltk.corpus import *\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "wordlemmatizer = WordNetLemmatizer()\n",
    "\n",
    "texts = pd.read_csv('SMSSpamCollection.tsv',sep=\"\\t\", quoting=csv.QUOTE_NONE, names=[\"label\", \"message\"])\n",
    "\n",
    "\n",
    "words = [wordlemmatizer.lemmatize(m.lower())  for w in texts.message for m in nltk.word_tokenize(w) if m not in stopwords and m.isalpha()]  \n",
    "all_words = nltk.FreqDist(w for w in words)\n",
    "featuresWord = all_words.most_common()\n",
    "featuresWord = [w for (w,freq) in featuresWord]\n",
    "featuresWord = featuresWord[:2000] \n",
    "\n",
    "\n",
    "def features(document): \n",
    "    document_words = set(document) \n",
    "    features = {}\n",
    "    for word in featuresWord:\n",
    "        features['contains({})'.format(word)] = (word in document_words) \n",
    "    return features\n",
    "\n",
    "\n",
    "ham = []\n",
    "for i in range(5570):\n",
    "    word = texts.message[i]\n",
    "    label = texts.label[i]\n",
    "    if(label == 'ham' and i != 0):\n",
    "        ham.append( (list(nltk.word_tokenize(word)), label))\n",
    "#documents[5]\n",
    "\n",
    "spam = []\n",
    "for i in range(5570):\n",
    "    word = texts.message[i]\n",
    "    label = texts.label[i]\n",
    "    if(label == 'spam' and i != 0):\n",
    "        spam.append( (list(nltk.word_tokenize(word)), label))\n",
    "    \n",
    "\n",
    "#documents = ham + spam\n",
    "#len(documents)\n",
    "#random.shuffle(documents)\n",
    "featuresetsham = [(features(d),c) for (d,c) in ham]\n",
    "featuresetsspam = [(features(d),c) for (d,c) in spam]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_set , test_set = featuresetsham[:3860] + featuresetsspam[:596]  ,  featuresetsham[3860:] + featuresetsspam[596:] \n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "#classifier = nltk.NaiveBayesClassifier.train(trainham)\n",
    "\n",
    "print ('Accuracy ' ,nltk.classify.accuracy(classifier, test_set))\n",
    "print(\"------------------------------------------\")\n",
    "classifier.show_most_informative_features(10) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
